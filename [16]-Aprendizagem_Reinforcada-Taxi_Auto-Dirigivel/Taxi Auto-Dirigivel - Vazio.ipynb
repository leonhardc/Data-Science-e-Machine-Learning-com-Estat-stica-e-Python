{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizagem Reinforçada\n",
    "## Taxi Auto-Dirigível\n",
    "O agente de Aprendizagem Reinforçada _(Reinforcement Learning)_ encontra um estado/cenário, e então toma uma ação de acordo com o estado/cenário atual. O objetivo é que o agente aprenda a pegar o passageiro numa posição e deixá-lo no destino.\n",
    "\n",
    "<img src=\"imagens/taxienv.png\" width=\"400px\" />\n",
    "\n",
    "* 5 x 5 = 25 possíveis posições\n",
    "* Posição atual do táxi (3,1) - Linha 3 e Coluna 1\n",
    "* 4 locais para pegar (pick up) e deixar (drop off) passageiros: R, G, Y B\n",
    "* Locais\n",
    "    * R (0,0) \n",
    "    * G (0,4) \n",
    "    * Y (4,0) \n",
    "    * B (4,3)\n",
    "* Posição do Passageiro sempre estará em AZUL\n",
    "* O destino do passageiro estará sempre em ROSA\n",
    "* Logo, no cenário atual o passageiro está no Y e deseja chegar no R\n",
    "* As possíveis posições do passageiro são os 4 locais, mais 1 da posição de dentro do táxi\n",
    "* Se a gente contabilizar todas as possíveis posições, teremos:\n",
    "    * Posições do Táxi (5x5) | 5 Posições do Passageiro | 4 destinos\n",
    "    * 5 x 5 x 5 x 4 = 500 estados/cenários possíveis\n",
    "    \n",
    "\n",
    "<table><tr>\n",
    "        <td>  \n",
    "        <ol>    \n",
    "            <center><h4>6 Possíveis Ações</h4></center>\n",
    "            <li>South (Sul)</li>\n",
    "            <li>North (Norte)</li>\n",
    "            <li>East  (Leste)</li>\n",
    "            <li>West  (Oeste)</li>\n",
    "            <li>Pickup (Pegar)</li>\n",
    "            <li>Dropoff (Deixar)</li>\n",
    "            </ol>\n",
    "        </td>\n",
    "        <td>\n",
    "        <img src=\"imagens/rosadosventos.jpeg\" width=\"200px\"/>\n",
    "        </td>    \n",
    "       </tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[toy_text] in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from gym[toy_text]) (1.20.3)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from gym[toy_text]) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from gym[toy_text]) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from gym[toy_text]) (4.8.1)\n",
      "Collecting pygame==2.1.0\n",
      "  Downloading pygame-2.1.0-cp39-cp39-win_amd64.whl (4.8 MB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (3.6.0)\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install gym[toy_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[110, 109, 106],\n",
       "         [110, 109, 106],\n",
       "         [124, 122, 122],\n",
       "         ...,\n",
       "         [108, 111, 109],\n",
       "         [108, 111, 109],\n",
       "         [118, 119, 119]],\n",
       " \n",
       "        [[110, 109, 106],\n",
       "         [110, 109, 106],\n",
       "         [124, 122, 122],\n",
       "         ...,\n",
       "         [108, 111, 109],\n",
       "         [108, 111, 109],\n",
       "         [118, 119, 119]],\n",
       " \n",
       "        [[114, 116, 115],\n",
       "         [114, 116, 115],\n",
       "         [126, 127, 126],\n",
       "         ...,\n",
       "         [112, 113, 111],\n",
       "         [112, 113, 111],\n",
       "         [118, 117, 115]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[116, 115, 116],\n",
       "         [116, 115, 116],\n",
       "         [106, 107, 108],\n",
       "         ...,\n",
       "         [113, 115, 114],\n",
       "         [113, 115, 114],\n",
       "         [117, 114, 117]],\n",
       " \n",
       "        [[116, 115, 116],\n",
       "         [116, 115, 116],\n",
       "         [106, 107, 108],\n",
       "         ...,\n",
       "         [113, 115, 114],\n",
       "         [113, 115, 114],\n",
       "         [117, 114, 117]],\n",
       " \n",
       "        [[115, 112, 112],\n",
       "         [115, 112, 112],\n",
       "         [119, 119, 117],\n",
       "         ...,\n",
       "         [123, 119, 118],\n",
       "         [123, 119, 118],\n",
       "         [114, 114, 117]]], dtype=uint8)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3', render_mode='rgb_array', new_step_api=True).env\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo é pegar um passageiro em um local e deixá-lo no destino requerido.\n",
    "* **Recebe +20 pontos** quando deixar um passageiro no destino correto\n",
    "* **Perder -1 ponto** a cada movimento que ele dá\n",
    "* **Perde -10 pontos** para tentativa de pegar ou deixar o passageiro numa posição ilegal\n",
    "\n",
    "O agente aprende a fazer 6 ações de 0-5\n",
    "* 0 = south\n",
    "* 1 = north\n",
    "* 2 = east\n",
    "* 3 = west\n",
    "* 4 = pickup\n",
    "* 5 = dropoff\n",
    "\n",
    "PS1: Quando o táxi está com um passageiro dentro, sua cor muda para **verde**. <br>\n",
    "PS2: Os 500 estados possíveis, numerados de 0-499 são uma codificação da posição do táxi, passageiro, e destino.\n",
    "<br>PS3: O táxi não consegue fazer nenhum movimento em direção a parede. Quando o faz, **perde -1** e não sai do lugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ações possíveis Discrete(6)\n",
      "Posições possíveis Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print('Ações possíveis', env.action_space)\n",
    "print('Posições possíveis', env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Index|Letra\n",
    "|---|---|\n",
    "|  0 | R  |   \n",
    "| 1  | G  |   \n",
    "| 2  | Y  |  \n",
    "| 3  | B  |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renderizar o cenário da ilustração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado 328\n"
     ]
    }
   ],
   "source": [
    "estado = env.encode(3,1,2,0) #Taxi linha, taxi coluna, idx passageiro, idx destino\n",
    "\n",
    "print('Estado', estado)\n",
    "env.s = estado\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabela da Recompensa\n",
    "<br>{ação: [(probabilidade, próximo_estado, recompensa, terminado)]\n",
    "<br>ação: [(probabilidade, próximo_estado, recompensa, terminado)]\n",
    "<br>ação: [(probabilidade, próximo_estado, recompensa, terminado)]\n",
    "<br>ação: [(probabilidade, próximo_estado, recompensa, terminado)]\n",
    "<br>ação: [(probabilidade, próximo_estado, recompensa, terminado)]\n",
    "<br>ação: [(probabilidade, próximo_estado, recompensa, terminado)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[box2d] in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from gym[box2d]) (1.20.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from gym[box2d]) (2.0.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\Leonardo\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Leonardo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-5yryv53p\\\\box2d-py_607b4f35e36141819f8aca0b3c6b8f82\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Leonardo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-5yryv53p\\\\box2d-py_607b4f35e36141819f8aca0b3c6b8f82\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Leonardo\\AppData\\Local\\Temp\\pip-wheel-zd2ybzqx'\n",
      "       cwd: C:\\Users\\Leonardo\\AppData\\Local\\Temp\\pip-install-5yryv53p\\box2d-py_607b4f35e36141819f8aca0b3c6b8f82\\\n",
      "  Complete output (28 lines):\n",
      "  Using setuptools (version 58.0.4).\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\Box2D\n",
      "  copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-3.9\\Box2D\n",
      "  copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-3.9\\Box2D\n",
      "  creating build\\lib.win-amd64-3.9\\Box2D\\b2\n",
      "  copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-3.9\\Box2D\\b2\n",
      "  running build_ext\n",
      "  building 'Box2D._Box2D' extension\n",
      "  swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "  swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "  Box2D\\Common\\b2Math.h(67) : Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "  Box2D\\Common\\b2Math.h(158) : Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "  Box2D\\Common\\b2Math.h(197) : Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "  Box2D\\Common\\b2Math.h(271) : Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "  Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "  Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "  Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "  Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "  Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for box2d-py\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\Leonardo\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Leonardo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-5yryv53p\\\\box2d-py_607b4f35e36141819f8aca0b3c6b8f82\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Leonardo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-5yryv53p\\\\box2d-py_607b4f35e36141819f8aca0b3c6b8f82\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Leonardo\\AppData\\Local\\Temp\\pip-record-ucb5yk9j\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\Leonardo\\anaconda3\\Include\\box2d-py'\n",
      "         cwd: C:\\Users\\Leonardo\\AppData\\Local\\Temp\\pip-install-5yryv53p\\box2d-py_607b4f35e36141819f8aca0b3c6b8f82\\\n",
      "    Complete output (28 lines):\n",
      "    Using setuptools (version 58.0.4).\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.9\n",
      "    creating build\\lib.win-amd64-3.9\\Box2D\n",
      "    copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-3.9\\Box2D\n",
      "    copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-3.9\\Box2D\n",
      "    creating build\\lib.win-amd64-3.9\\Box2D\\b2\n",
      "    copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-3.9\\Box2D\\b2\n",
      "    running build_ext\n",
      "    building 'Box2D._Box2D' extension\n",
      "    swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "    swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "    Box2D\\Common\\b2Math.h(67) : Warning 302: Identifier 'b2Vec2' redefined by %extend (ignored),\n",
      "    Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "    Box2D\\Common\\b2Math.h(158) : Warning 302: Identifier 'b2Vec3' redefined by %extend (ignored),\n",
      "    Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "    Box2D\\Common\\b2Math.h(197) : Warning 302: Identifier 'b2Mat22' redefined by %extend (ignored),\n",
      "    Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "    Box2D\\Common\\b2Math.h(271) : Warning 302: Identifier 'b2Mat33' redefined by %extend (ignored),\n",
      "    Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "    Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "    Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "    Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "    Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "    error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\Leonardo\\anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Leonardo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-5yryv53p\\\\box2d-py_607b4f35e36141819f8aca0b3c6b8f82\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Leonardo\\\\AppData\\\\Local\\\\Temp\\\\pip-install-5yryv53p\\\\box2d-py_607b4f35e36141819f8aca0b3c6b8f82\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Leonardo\\AppData\\Local\\Temp\\pip-record-ucb5yk9j\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\Leonardo\\anaconda3\\Include\\box2d-py' Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from gym[box2d]) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from gym[box2d]) (4.12.0)\n",
      "Requirement already satisfied: pygame==2.1.0 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from gym[box2d]) (2.1.0)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from gym[box2d]) (4.0.2)\n",
      "Collecting box2d-py==2.3.5\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\leonardo\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.6.0)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n",
      "Failed to build box2d-py\n",
      "Installing collected packages: box2d-py\n",
      "    Running setup.py install for box2d-py: started\n",
      "    Running setup.py install for box2d-py: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leonardo\\anaconda3\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\Leonardo\\anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\Leonardo\\anaconda3\\lib\\site-packages\\gym\\core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 out of 99\n",
      "step: 1 out of 99\n",
      "step: 2 out of 99\n",
      "step: 3 out of 99\n",
      "step: 4 out of 99\n",
      "step: 5 out of 99\n",
      "step: 6 out of 99\n",
      "step: 7 out of 99\n",
      "step: 8 out of 99\n",
      "step: 9 out of 99\n",
      "step: 10 out of 99\n",
      "step: 11 out of 99\n",
      "step: 12 out of 99\n",
      "step: 13 out of 99\n",
      "step: 14 out of 99\n",
      "step: 15 out of 99\n",
      "step: 16 out of 99\n",
      "step: 17 out of 99\n",
      "step: 18 out of 99\n",
      "step: 19 out of 99\n",
      "step: 20 out of 99\n",
      "step: 21 out of 99\n",
      "step: 22 out of 99\n",
      "step: 23 out of 99\n",
      "step: 24 out of 99\n",
      "step: 25 out of 99\n",
      "step: 26 out of 99\n",
      "step: 27 out of 99\n",
      "step: 28 out of 99\n",
      "step: 29 out of 99\n",
      "step: 30 out of 99\n",
      "step: 31 out of 99\n",
      "step: 32 out of 99\n",
      "step: 33 out of 99\n",
      "step: 34 out of 99\n",
      "step: 35 out of 99\n",
      "step: 36 out of 99\n",
      "step: 37 out of 99\n",
      "step: 38 out of 99\n",
      "step: 39 out of 99\n",
      "step: 40 out of 99\n",
      "step: 41 out of 99\n",
      "step: 42 out of 99\n",
      "step: 43 out of 99\n",
      "step: 44 out of 99\n",
      "step: 45 out of 99\n",
      "step: 46 out of 99\n",
      "step: 47 out of 99\n",
      "step: 48 out of 99\n",
      "step: 49 out of 99\n",
      "step: 50 out of 99\n",
      "step: 51 out of 99\n",
      "step: 52 out of 99\n",
      "step: 53 out of 99\n",
      "step: 54 out of 99\n",
      "step: 55 out of 99\n",
      "step: 56 out of 99\n",
      "step: 57 out of 99\n",
      "step: 58 out of 99\n",
      "step: 59 out of 99\n",
      "step: 60 out of 99\n",
      "step: 61 out of 99\n",
      "step: 62 out of 99\n",
      "step: 63 out of 99\n",
      "step: 64 out of 99\n",
      "step: 65 out of 99\n",
      "step: 66 out of 99\n",
      "step: 67 out of 99\n",
      "step: 68 out of 99\n",
      "step: 69 out of 99\n",
      "step: 70 out of 99\n",
      "step: 71 out of 99\n",
      "step: 72 out of 99\n",
      "step: 73 out of 99\n",
      "step: 74 out of 99\n",
      "step: 75 out of 99\n",
      "step: 76 out of 99\n",
      "step: 77 out of 99\n",
      "step: 78 out of 99\n",
      "step: 79 out of 99\n",
      "step: 80 out of 99\n",
      "step: 81 out of 99\n",
      "step: 82 out of 99\n",
      "step: 83 out of 99\n",
      "step: 84 out of 99\n",
      "step: 85 out of 99\n",
      "step: 86 out of 99\n",
      "step: 87 out of 99\n",
      "step: 88 out of 99\n",
      "step: 89 out of 99\n",
      "step: 90 out of 99\n",
      "step: 91 out of 99\n",
      "step: 92 out of 99\n",
      "step: 93 out of 99\n",
      "step: 94 out of 99\n",
      "step: 95 out of 99\n",
      "step: 96 out of 99\n",
      "step: 97 out of 99\n",
      "step: 98 out of 99\n",
      "step: 99 out of 99\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# create Taxi environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# create a new instance of taxi, and get the initial state\n",
    "state = env.reset()\n",
    "\n",
    "num_steps = 99\n",
    "for s in range(num_steps+1):\n",
    "    print(f\"step: {s} out of {num_steps}\")\n",
    "\n",
    "    # sample a random action from the list of available actions\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # perform this action on the environment\n",
    "    env.step(action)\n",
    "\n",
    "    # print the new state\n",
    "    env.render()\n",
    "\n",
    "# end this instance of the taxi environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solucionando Sem Aprendizagem Reinforçada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com Aprendizagem Reinforçada\n",
    "\n",
    "#### Q-learning\n",
    "Essencialmente, o agente vai aprender através das recompensas(positivas e negativas) com um tempo a tomar a melhor decisão para um determinado estado.\n",
    "\n",
    "* Temos a tabela da recompensa P que é de onde o agente vai aprender, ao tomar uma ação no estado atual e observando a recompensa/punição, atualiza o valor-Q (Q-value).\n",
    "* O valor-Q para um estado/cenário representa a \"qualidade\" da ação que ele irá tomar\n",
    "\n",
    "Os valores-Q são inicializados de forma aleatória, e o agente se expõe ao ambiente, onde recebe diferentes recompensas (positivas e negativas) ao tomar diferentes ações, de forma que os valores-Q são atualizados usando a seguinte fórmula:\n",
    "\n",
    "$$Q({\\small estado}, {\\small ação}) = (1 - \\alpha) \\cdot Q({\\small estado}, {\\small ação}) + \\alpha \\Big({\\small recompensa} + {\\gamma \\max}_{a} Q({\\small próximo \\ estado}, {\\small todas \\ ações})\\Big)$$\n",
    "\n",
    "Onde:\n",
    "- $\\Large \\alpha$ (Alpha) é a taxa de aprendizagem (entre 0 e 1)\n",
    "- $\\Large \\gamma$ (Gamma) é o fator de desconto também (entre 0 e 1), que significa o quanto de importância a gente quer dar para uma recompensa. De forma que 0 faz com que o agente se preocupe apenas com a recompensa imediata. O ideal é que o agente tome as ações considerando as recompensas do estado atual, e o máximo de recompensa para o próximo estado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabela-Q\n",
    "<img src=\"imagens/qtable.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Tabela-Q tem seus valores inicializados como 0, e depois vão sendo atualizados conforme o agente vai tomando ações no ambiente e obtendo o máximo de recompensas\n",
    "\n",
    "### Resumindo\n",
    "* Inicializa a tabela-Q com zeros\n",
    "* Começa a explorar o ambiente com ações, seleciona uma de todas as ações possíveis no estado atual ($Es_{1}$)\n",
    "* Vai para o próximo estado ($Es_{2}$) como resultado da ação ($A_{1}$)\n",
    "* De todas as possíveis ações no estado ($Es_{2}$) seleciona a que possui o maior valor-Q\n",
    "* Atualiza a tabela-Q usando a equação\n",
    "* Define o próximo estado como o estado atual\n",
    "* Se o objetivo é alcançado, termina, senão, repete o processo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando valores aprendidos\n",
    "Depois de explorar ações aleatórias, os valores-Q tendem a se divergirem, de forma que o agente vai poder escolher a melhor ação possível para um determinado estado.\n",
    "\n",
    "Existe um meio termo entre explorar (escolher uma ação aleatória) e usufruir (escolher ações baseadas em valores-Q já aprendidos/treinados). Nós queremos impedir o agente de ficar toda vez fazendo os exatos movimentos, e possivelmente se super-adequando _(overfitting)_. Para evitar isso, usamos mais um parâmetro chamado $\\Large \\epsilon$ \"epsilon\" para equilibrar essas ações durante o treino do agente.\n",
    "\n",
    "Ao invés de apenas selecionar o melhor valor-Q, algumas vezes vamos explorar novas ações. Um epsilon pequeno trás mais punições (em média), o que é natural, uma vez que estamos explorando tomando ações aleatórias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já treinamos o Agente, não precisamos mais explorar. Vamos apenas selecionar sempre a melhor ação escolhendo o melhor valor-Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimizações de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha, gamma e epsilon foram definidos baseados na intuição, mas existem melhores formas de escolher os melhores parâmetros e ter um desempenho melhor.\n",
    "\n",
    "* $\\Large \\alpha$ (Alpha) - (Taxa de aprendizagem) = Deveria diminuir com o tempo, para o agente aprender cada vez mais e mais\n",
    "* $\\Large \\gamma$ (Gamma) - Quanto mais próximo você está do objetivo final, maior deveria ser a preferência para a recompensa imediata\n",
    "* $\\Large \\epsilon$ (Epsilon) - Quanto mais experiência tem o agente, menos precisará explorar. Logo, o epsilon deve diminuir com o tempo.\n",
    "\n",
    "* Poderíamos aplicar uma pesquisa pelos melhores parâmetros, similar ao GridSearch que vimos para os modelos preditivos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fontes (em inglês):\n",
    "* https://gym.openai.com/\n",
    "* https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
